# Deploying Kai (Beta)

This guide gets you from zero to a running beta in minutes. It uses a Turborepo with a single Next.js app (`apps/web`) that also serves the API (BFF). Now includes streaming responses and a post-process API for better vocabulary.

## Prerequisites
- Node 18+ and pnpm 9: `corepack enable && corepack prepare pnpm@9 --activate`
- An OpenAI API key (optional for mock mode). Create at https://platform.openai.com/

## Local Development
1. Clone and install
   - `pnpm install`
2. Configure environment
   - Copy `.env.example` → `apps/web/.env.local`
   - Set `OPENAI_API_KEY` and `OPENAI_MODEL` (e.g., `gpt-4o-mini`). If omitted, the API returns a mock reply.
3. Run the app (streaming enabled)
   - `pnpm dev`
   - Open http://localhost:3000

## Vercel Deploy (Recommended)
1. Push this repo to GitHub/GitLab.
2. In Vercel, “New Project” → Import your repo.
3. Framework Preset: Turborepo. Root directory: repository root.
4. Build command (auto): `pnpm turbo run build --filter=@kai/web`
5. Output directory: default (Vercel detects Next.js output).
6. Environment Variables (Project Settings → Environment Variables):
   - `OPENAI_API_KEY` = your key
   - `OPENAI_MODEL` = `gpt-4o-mini` (or preferred)
   - `KAI_DEFAULT_TARGET_LANG` = `en`
   - `KAI_DEFAULT_LEVEL` = `intermediate`
7. Deploy. Visit the generated URL.

## Notes
- Runtime: The API endpoints run on Vercel Edge (fast, serverless). Secrets are never exposed to the browser.
- Cost control: Since all calls pass the server route, you can later add rate limiting or token budgeting there.
- Extensibility: Core logic is in `packages/` and is UI/framework-agnostic for future desktop/mobile.
 - Endpoints:
   - `POST /api/stream` — streams only the learner-facing answer (text/plain)
   - `POST /api/postprocess` — returns JSON with corrections/hints/vocab
   - `POST /api/translate` — English → Chinese translation
   - `POST /api/dict` — contextual dictionary lookup
   - `GET|POST /api/chats` — optional Supabase-backed chat list (create/delete)
   - `POST /api/messages` — optional Supabase-backed message insert

## Optional: Enable Supabase Persistence
You can keep using localStorage, or enable server persistence with Supabase.

1) Create tables (SQL)
```
create table if not exists public.chats (
  id uuid primary key default gen_random_uuid(),
  title text not null,
  created_at timestamptz not null default now()
);

create table if not exists public.messages (
  id uuid primary key default gen_random_uuid(),
  chat_id uuid references public.chats(id) on delete cascade,
  role text check (role in ('user','assistant')) not null,
  content text not null,
  meta jsonb,
  created_at timestamptz not null default now()
);
```

2) Set environment variables in Vercel
- `SUPABASE_URL` = your project URL (https://xxxx.supabase.co)
- `SUPABASE_SERVICE_ROLE` = service role key (server only; keep secret)

3) RLS (optional now): You can later enable RLS and issue anon keys with policies. For beta, service role in server routes is simplest.

4) Storage bucket for uploads
- In Supabase → Storage, create a bucket named `kai-files` (or your chosen name) and mark it Public for simple public URLs.
- Add env: `SUPABASE_STORAGE_BUCKET=kai-files`.
- The upload endpoint expects form-data with fields: `chatId` and one or more `file` entries.
- Public URL format: `https://<project>.supabase.co/storage/v1/object/public/kai-files/<path>`.

## Troubleshooting
- Build fails on Vercel: ensure `pnpm-lock.yaml` is generated by running `pnpm install` locally and pushing the lockfile.
- 500 from `/api/chat`: verify `OPENAI_API_KEY` and `OPENAI_MODEL` are set. Without a key, the app uses a mock client.
- Quota errors: if OpenAI returns `insufficient_quota`, the streaming endpoint falls back to a friendly mock stream and post-processing returns mock vocab so the UI remains usable. Replace the key or top-up billing to restore live output.
- TTS not audible: some browsers require user interaction; click “Play” on a response, and ensure system audio is enabled.
